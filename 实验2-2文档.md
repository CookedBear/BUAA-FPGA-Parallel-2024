### 1. 实验2-2：PYNQ与深度学习

### 实验名称

PYNQ与深度学习

### 实验目的

1. 了解FPGA平台所能运行的神经网络类型（BNN、QNN）
1. 掌握PYNQ平台下常用FPGA网络的基本搭建方法

### 实验任务

1. 配置QNN实验环境
1. 使用`Tiny-Yolo`算法进行目标检测

### 实验结果

![img](https://course.educg.net/userfiles/markdown/exp/2021_3/84467ll1616757007.png)

### 实验分析

1. 涉及知识点：
   - 深度学习目标检测的基本原理
   - 神经网络量化原理
1. 涉及技能点：
   - 掌握QNN（Quantized Neural Network，量化神经网络）FPGA运行环境的配置方法
   - 使用Tiny-YOLO算法进行目标检测

### 实验步骤

1. #### 配置QNN运行环境

   **此部分在开发板上完成**

   1. 修改QNN安装配置

      - 拷贝`rapidjson`到`qnn/src/library`目录下

        ```shell
        cp -r QNN-MO-PYNQ-master/library/rapidjson QNN-MO-PYNQ-master/qnn/src/library
        ```

      - 部署`darknet`包

        ```shell
        tar -zxvf darknet.tar.gz
        ```

        以上工作建议在xilinx用户权限下完成

      - 修改配置文件`QNN-MO-PYNQ-master/setup.py`内容： ①由于当前环境不为标准`PYNQ-Z1`或`PYNQ-Z2`，因此，需要将运行过程中检查`PLATFORM`版本的代码注释掉，并将`PLATFORM`设置为`pynqZ1-Z2`才可正常运行； ②关闭`git clone`操作，将课程提供的darknet文件上传到开发板并解压，调整`setup.py`中的`darknet`路径。

      - 修改`QNN-MO-PYNQ-master/qnn`下`dorefanet.py`、`tineryolo.py`中的`PLATFORM`为`pynqZ1-Z2`

   1. 生成`lib_hw.so`库文件

      ```shell
      cd QNN-MO-PYNQ-master/qnn/src/network
      make lib_hw
      cp QNN-MO-PYNQ-master/qnn/src/network/output/lib_hw.so QNN-MO-PYNQ-master/qnn/libraries/pynqZ1-Z2/lib_hw.so      
      ```

   1. 安装QNN

      ```
      cd QNN-MO-PYNQ-master
      python3 setup.py install
      ```

1. #### 使用Tiny-Yolo进行目标检测

   通过以下示例来进一步了解图片中的目标检测工作，本次实验所使用到的推理网络是`Tiny-Yolo`的一个变体，其网络结构如下图所示。其中粉色部分的层被量化为1位权值和3位激活值，经过量化后的层可运行在硬件加速器中，而其他层将运行在python中。

   > **输入图片的尺寸：**一般用 n×n�×�表示输入的image大小。
   >
   >  
   >
   > **卷积核的大小：**一般用f×f�×�表示卷积核的大小。
   >
   >  
   >
   > **填充（Padding）：**一般用p�来表示填充大小。
   >
   >  
   >
   > **步长(Stride)：**一般用s�来表示步长大小。
   >
   >  
   >
   > **输出图片的尺寸：**一般用o�来表示。
   >
   >  
   >
   > 如果已知n�、f�、p�、s�可以求得o� ，**计算公式如下：**o=⌊n+2p−fs⌋+1�=⌊�+2�−��⌋+1
   >
   >  
   >
   > 其中"![[公式]](https://www.zhihu.com/equation?tex=%5Clfloor+%5C+%5Crfloor)"是向下取整符号，用于结果不是整数时进行向下取整。 ![img](https://course.educg.net/userfiles/markdown/exp/2021_3/84467ll1616757205.png)

   注意事项：直接使用`QNN-MO-PYNQ-master/qnn/notebooks`下的`tiny-yolo-iamge.ipynb`，以下是`tiny-yolo-iamge.ipynb`的代码解析。

   1. ##### 导入所需依赖

      ```python
      import sys
      import os, platform
      import json
      import numpy as np
      import cv2
      import ctypes
      
      from PIL import Image
      from datetime import datetime
      
      import qnn
      from qnn import TinierYolo
      from qnn import utils 
      sys.path.append("/opt/darknet/python/")  # darknet/python路径
      from darknet import *
      
      from matplotlib import pyplot as plt
      %matplotlib inline
      ```

   1. ##### 实例化分类器

      在创建分类器时会自动将比特流文件下载至设备。所有其他的初始化工作都将通过Darknet框架执行。

      ```python
      classifier = TinierYolo()
      classifier.init_accelerator()
      net = classifier.load_network(json_layer="/usr/local/lib/python3.6/dist-packages/qnn/params/tinier-yolo-layers.json") # qnn/params/tinier-yolo-layers.json路径
      
      conv0_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/tinier-yolo-conv0-W.npy', encoding="latin1", allow_pickle=True) # qnn/params/tinier-yolo-conv0-W.npy路径
      conv0_weights_correct = np.transpose(conv0_weights, axes=(3, 2, 1, 0))
      conv8_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/tinier-yolo-conv8-W.npy', encoding="latin1", allow_pickle=True) # qnn/params/tinier-yolo-conv8-W.npy路径
      conv8_weights_correct = np.transpose(conv8_weights, axes=(3, 2, 1, 0))
      conv0_bias = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/tinier-yolo-conv0-bias.npy', encoding="latin1", allow_pickle=True) # qnn/params/tinier-yolo-conv0-bias.npy路径
      conv0_bias_broadcast = np.broadcast_to(conv0_bias[:,np.newaxis], (net['conv1']['input'][0],net['conv1']['input'][1]*net['conv1']['input'][1]))
      conv8_bias = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/tinier-yolo-conv8-bias.npy', encoding="latin1", allow_pickle=True) # qnn/params/tinier-yolo-conv8-bias.npy路径
      conv8_bias_broadcast = np.broadcast_to(conv8_bias[:,np.newaxis], (125,13*13))
      file_name_cfg = c_char_p("/usr/local/lib/python3.6/dist-packages/qnn/params/tinier-yolo-bwn-3bit-relu-nomaxpool.cfg".encode())              # qnn/params/tinier-yolo-bwn-3bit-relu-nomaxpool.cfg路径
      
      net_darknet = lib.parse_network_cfg(file_name_cfg)
      ```

   1. ##### 针对输入图片进行物体检测

      设置`img_folder`变量，对指定目录下的图像随机抽取一张进行物体检测。

      目前提供的参数已经在PASCAL VOC（可视对象类）上进行了训练，可支持识别图像中的20类物体对象，包括：

      人：人

      动物：鸟，猫，牛，狗，马，绵羊

      车辆：飞机，自行车，轮船，公共汽车，汽车，摩托车，火车

      室内：瓶子，椅子，餐桌，盆栽，沙发，电视/显示器

      ```python
      img_folder = './yoloimages/'
      img_file = os.path.join(img_folder, random.choice(os.listdir(img_folder)))
      file_name = c_char_p(img_file.encode())
      
      img = load_image(file_name,0,0)
      img_letterbox = letterbox_image(img,416,416)
      img_copy = np.copy(np.ctypeslib.as_array(img_letterbox.data, (3,416,416)))
      img_copy = np.swapaxes(img_copy, 0,2)
      free_image(img)
      free_image(img_letterbox)
      
      im = Image.open(img_file)
      im
      ```

   1. ##### 在Python中运行第一层卷积层

      本次神经网络的第一层是未经过量化处理的，这使得他无法运行在硬件加速器上（硬件加速器只支持量化后的算法）。Python通过numpy提供了一个后端来执行卷积和其他矩阵操作。为方便用户使用，utils类中提供了基本操作（卷积层、阈值设置、relu和全连接层）。

      ```python
      start = datetime.now()
      img_copy = img_copy[np.newaxis, :, :, :]
          
      conv0_ouput = utils.conv_layer(img_copy,conv0_weights_correct,b=conv0_bias_broadcast,stride=2,padding=1)
      conv0_output_quant = conv0_ouput.clip(0.0,4.0)
      conv0_output_quant = utils.quantize(conv0_output_quant/4,3)
      end = datetime.now()
      micros = int((end - start).total_seconds() * 1000000)
      print("First layer SW implementation took {} microseconds".format(micros))
      print(micros, file=open('timestamp.txt', 'w'))
      ```

   1. ##### 硬件

      核心层经过训练过程中的量化后，可执行在硬件加速器上，硬件加速器是由多层次的数据流实现组成（其中包括卷积和池化）。管理程序通过分析网络拓扑，管理加速器的执行序列。

      ```python
      out_dim = net['conv7']['output'][1]
      out_ch = net['conv7']['output'][0]
      
      conv_output = classifier.get_accel_buffer(out_ch, out_dim)
      conv_input = classifier.prepare_buffer(conv0_output_quant*7);
      
      start = datetime.now()
      classifier.inference(conv_input, conv_output)
      end = datetime.now()
      
      conv7_out = classifier.postprocess_buffer(conv_output)
      
      micros = int((end - start).total_seconds() * 1000000)
      print("HW implementation took {} microseconds".format(micros))
      print(micros, file=open('timestamp.txt', 'a'))
      ```

   1. ##### 在Python上运行最后一个卷积层

      ```python
      start = datetime.now()
      conv7_out_reshaped = conv7_out.reshape(out_dim,out_dim,out_ch)
      conv7_out_swapped = np.swapaxes(conv7_out_reshaped, 0, 1) # exp 1
      conv7_out_swapped = conv7_out_swapped[np.newaxis, :, :, :] 
      
      conv8_output = utils.conv_layer(conv7_out_swapped,conv8_weights_correct,b=conv8_bias_broadcast,stride=1)  
      conv8_out = conv8_output.ctypes.data_as(ctypes.POINTER(ctypes.c_float))
      
      end = datetime.now()
      micros = int((end - start).total_seconds() * 1000000)
      print("Last layer SW implementation took {} microseconds".format(micros))
      print(micros, file=open('timestamp.txt', 'a'))
      ```

   1. ##### 使用Darknet绘制检测框

      针对图像的进一步处理（绘制包围框）在`darknet`中以Python的方式进行提供

      ```python
      lib.forward_region_layer_pointer_nolayer(net_darknet,conv8_out)
      tresh = c_float(0.3)
      tresh_hier = c_float(0.5)
      file_name_out = c_char_p("/home/xilinx/jupyter_notebooks/qnn/detection".encode())
      file_name_probs = c_char_p("/home/xilinx/jupyter_notebooks/qnn/probabilities.txt".encode())
      file_names_voc = c_char_p("/opt/darknet/data/voc.names".encode())
      darknet_path = c_char_p("/opt/darknet/".encode())
      lib.draw_detection_python(net_darknet, file_name, tresh, tresh_hier,file_names_voc, darknet_path, file_name_out, file_name_probs);
      
      #Print probabilities
      file_content = open(file_name_probs.value,"r").read().splitlines()
      detections = []
      for line in file_content[0:]:
          name, probability = line.split(": ")
          detections.append((probability, name))
      for det in sorted(detections, key=lambda tup: tup[0], reverse=True):
          print("class: {}\tprobability: {}".format(det[1], det[0]))
      ```

   1. ##### 结果展示

      此步将展示物体识别的结果，将通过在原图上绘制包围框的形式

      ```python
      res = Image.open(file_name_out.value.decode() + ".png")
      res
      ```

   1. ##### 性能分析

      此步展示了软硬件混合设计的执行情况，并在图中分析了延迟和吞吐量方面的性能

      ```python
      array = np.loadtxt('timestamp.txt')
      array = list(map(lambda x: x/1000000, array))
      
      MOPS=[171.3254,4385.99]
      TIME=[array[0]+array[2],array[1]]
      LABELS=["SW", "HW"]
      
      f, ((ax1, ax2, ax3)) = plt.subplots(1, 3, sharex='col', sharey='row', figsize=(15,2))
      x_pos = np.arange(len(LABELS))
      
      plt.yticks(x_pos, LABELS)
      ax1.invert_yaxis()
      ax1.set_xlabel("执行时间 [ms]")
      ax1.set_ylabel("Platform")
      ax1.barh(x_pos, TIME, height=0.6, color='g', zorder=3)
      ax1.grid(zorder=0)
      
      ax2.invert_yaxis()
      ax2.set_xlabel("计算量 [MOPS]")
      ax2.barh(x_pos, MOPS, height=0.6, color='y', zorder=3)
      ax2.grid(zorder=0)
      
      MOPSS=[MOPS[i]/TIME[i] for i in range(len(MOPS))]
      x_pos = np.arange(len(LABELS))
      ax3.barh(x_pos, MOPSS, height=0.6, color='r', zorder=3)
      ax3.invert_yaxis()
      ax3.set_xlabel("性能 [MOPS/s]")
      ax3.grid(zorder=0)
      plt.show()
      ```

   1. ##### 重置设备

      ```python
      classifier.deinit_accelerator()
      from pynq import Xlnk
      
      xlnk = Xlnk();
      xlnk.xlnk_reset()
      ```

### 问题思考

1. 自行下载调试`BNN`环境，完成`notebooks/CNV-BNN_Cifar10.ipynb`的实验结果

1. 根据配置构建比特流文件

   1. 配置环境变量

      ```
      XILINX_BNN_ROOT配置为“当前BNN-PYNQ路径/bnn/src”
      PATH配置为“原有PATH内容:/tools/Xilinx/Vivado/2018.3/bin”
      ```

   1. 使用以下命令运行脚本制作镜像

      ```
      cd BNN-PYNQ-master/bnn/src/network
      ./make-hw.sh cnvW2A2 pynqZ1-Z2 a
      ```

   > 请将此过程中生成的`output/hls-syn/vivado_hls.log`文件和`output/vivado/W2A2-pynqZ1-Z2/W2A2-pynqZ1-Z2.runs/impl_1/runme.log`与本次实验报告共同打包上传
   >
   >  
   >
   > 注：
   >
   > 1. 官方样例生成环境为`Vivado 2018.2`，在线实验平台所使用的为`Vivado 2018.3`，需要修改若干配置
   > 1. 可通过在vivado console中使用`source xxx.tcl`命令执行tcl脚本
   > 1. 此部分在**桌面环境**完成

### 参考资料

- darknet仓库：https://github.com/pjreddie/darknet
- QNN仓库：https://github.com/Xilinx/QNN-MO-PYNQ
- BNN仓库：https://github.com/Xilinx/BNN-PYNQ
- YOLO算法官网：https://pjreddie.com/darknet/yolo/

### 相关资源

- QNN工程：[QNN-MO-PYNQ-master.tar.gz](https://course.educg.net/userfiles/markdown/exp/2022_3/8283ll1647516144.gz)
- Darknet框架：[darknet.tar.gz](https://course.educg.net/userfiles/markdown/exp/2022_3/8283ll1647516177.gz)
- 实验报告模板：[实验2-2：实验报告模板.doc](https://course.educg.net/userfiles/markdown/exp/2022_3/8283ll1647830427.doc)